# Обучение модели

# Архитектура модели
- ruBert-base / ruBert-large
- полносвязный слой с Softmax, для задачи Token Classification

# Датасет
100 предложений, размеченных экспертом на русском языке.
Подробную информацию о данных, как они были подготовлены, можно найти [здесь.](../data-preparation/README.md)

# Процесс обучения
- кодирование входного потока данных при помощи BPE
- кодирование вектора таргетов (первый полученный из слова токен - класс, остальные - 0)
- обучение на задачу Token Classification

# Результаты
Weighted F-1 Score = 0.87
Модель опубликована на бесплатном хостинге: [Hugging Face](https://huggingface.co/disk0dancer/rubert-base-finetuned-pos)